 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# New imports for added functionality\n",
    "# Make sure to install them: \n",
    "# pip install mixpanel-api tavily-python newsapi-python python-dotenv openai\n",
    "from mixpanel import Mixpanel\n",
    "from newsapi import NewsApiClient\n",
    "from openai import AsyncOpenAI\n",
    "from tavily import TavilyClient\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\"\"\"\n",
    "This file provides a simple forecasting bot built from the ground up,\n",
    "modified to use OpenRouter for LLM access, Tavily and NewsAPI for research,\n",
    "and Mixpanel for analytics.\n",
    "\n",
    "This template assumes you have API keys for:\n",
    "- OpenRouter (for LLM access)\n",
    "- Metaculus (for posting questions)\n",
    "- Tavily (for web search)\n",
    "- NewsAPI (for news articles)\n",
    "- Mixpanel (for event tracking)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "######################### CONSTANTS #########################\n",
    "# --- Core Settings ---\n",
    "SUBMIT_PREDICTION = True  # Set to True to publish your predictions to Metaculus\n",
    "USE_EXAMPLE_QUESTIONS = False  # Set to True to forecast example questions\n",
    "NUM_RUNS_PER_QUESTION = 3  # The median forecast is taken between N runs\n",
    "SKIP_PREVIOUSLY_FORECASTED_QUESTIONS = True\n",
    "RUN_ID = str(uuid.uuid4()) # Generate a unique ID for this script run for tracking\n",
    "\n",
    "# --- Environment Variables & API Keys ---\n",
    "METACULUS_TOKEN = os.getenv(\"METACULUS_TOKEN\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "NEWSAPI_API_KEY = os.getenv(\"NEWSAPI_API_KEY\")\n",
    "MIXPANEL_TOKEN = os.getenv(\"MIXPANEL_TOKEN\")\n",
    "\n",
    "# --- API Clients ---\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY) if TAVILY_API_KEY else None\n",
    "newsapi_client = NewsApiClient(api_key=NEWSAPI_API_KEY) if NEWSAPI_API_KEY else None\n",
    "mixpanel_client = Mixpanel(MIXPANEL_TOKEN) if MIXPANEL_TOKEN else None\n",
    "\n",
    "# --- OpenRouter Configuration ---\n",
    "# Specify the model you want to use from OpenRouter\n",
    "# Find model strings here: https://openrouter.ai/docs#models\n",
    "OPENROUTER_MODEL = \"anthropic/claude-3.5-sonnet\"\n",
    "APP_URL = \"https://metaculus.com/your-bot-profile\" # For OpenRouter headers\n",
    "APP_TITLE = \"Metaculus Forecasting Bot\" # For OpenRouter headers\n",
    "\n",
    "# --- Tournament & Question IDs ---\n",
    "# The tournament IDs below can be used for testing your bot.\n",
    "FALL_2025_AI_BENCHMARKING_ID = \"fall-aib-2025\"\n",
    "TOURNAMENT_ID = FALL_2025_AI_BENCHMARKING_ID\n",
    "\n",
    "# The example questions can be used for testing your bot.\n",
    "EXAMPLE_QUESTIONS = [\n",
    "    (578, 578),  # Human Extinction - Binary\n",
    "    (14333, 14333),  # Age of Oldest Human - Numeric\n",
    "    (22427, 22427),  # Number of New Leading AI Labs - Multiple Choice\n",
    "    (38195, 38880), # Number of US Labor Strikes Due to AI in 2029 - Discrete\n",
    "]\n",
    "\n",
    "\n",
    "######################### HELPER FUNCTIONS #########################\n",
    "\n",
    "AUTH_HEADERS = {\"headers\": {\"Authorization\": f\"Token {METACULUS_TOKEN}\"}}\n",
    "API_BASE_URL = \"https://www.metaculus.com/api\"\n",
    "\n",
    "def track_event(event_name: str, properties: dict = None):\n",
    "    \"\"\"\n",
    "    Tracks an event using Mixpanel if the client is configured.\n",
    "    \"\"\"\n",
    "    if not mixpanel_client:\n",
    "        return\n",
    "    \n",
    "    event_properties = {\n",
    "        'run_id': RUN_ID,\n",
    "        'model': OPENROUTER_MODEL,\n",
    "        'timestamp': datetime.datetime.utcnow().isoformat()\n",
    "    }\n",
    "    if properties:\n",
    "        event_properties.update(properties)\n",
    "    \n",
    "    try:\n",
    "        mixpanel_client.track(distinct_id=RUN_ID, event_name=event_name, properties=event_properties)\n",
    "        print(f\"[Mixpanel] Tracked event: {event_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Mixpanel] Error tracking event: {e}\")\n",
    "\n",
    "def post_question_comment(post_id: int, comment_text: str) -> None:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE_URL}/comments/create/\",\n",
    "        json={\n",
    "            \"text\": comment_text,\n",
    "            \"parent\": None,\n",
    "            \"included_forecast\": True,\n",
    "            \"is_private\": True,\n",
    "            \"on_post\": post_id,\n",
    "        },\n",
    "        **AUTH_HEADERS,  # type: ignore\n",
    "    )\n",
    "    if not response.ok:\n",
    "        raise RuntimeError(response.text)\n",
    "\n",
    "\n",
    "def post_question_prediction(question_id: int, forecast_payload: dict) -> None:\n",
    "    url = f\"{API_BASE_URL}/questions/forecast/\"\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=[\n",
    "            {\n",
    "                \"question\": question_id,\n",
    "                **forecast_payload,\n",
    "            },\n",
    "        ],\n",
    "        **AUTH_HEADERS,  # type: ignore\n",
    "    )\n",
    "    print(f\"Prediction Post status code: {response.status_code}\")\n",
    "    if not response.ok:\n",
    "        raise RuntimeError(response.text)\n",
    "\n",
    "\n",
    "def create_forecast_payload(\n",
    "    forecast: float | dict[str, float] | list[float],\n",
    "    question_type: str,\n",
    ") -> dict:\n",
    "    if question_type == \"binary\":\n",
    "        return {\n",
    "            \"probability_yes\": forecast,\n",
    "            \"probability_yes_per_category\": None,\n",
    "            \"continuous_cdf\": None,\n",
    "        }\n",
    "    if question_type == \"multiple_choice\":\n",
    "        return {\n",
    "            \"probability_yes\": None,\n",
    "            \"probability_yes_per_category\": forecast,\n",
    "            \"continuous_cdf\": None,\n",
    "        }\n",
    "    # numeric or date\n",
    "    return {\n",
    "        \"probability_yes\": None,\n",
    "        \"probability_yes_per_category\": None,\n",
    "        \"continuous_cdf\": forecast,\n",
    "    }\n",
    "\n",
    "\n",
    "def list_posts_from_tournament(\n",
    "    tournament_id: int | str = TOURNAMENT_ID, offset: int = 0, count: int = 50\n",
    ") -> list[dict]:\n",
    "    url_qparams = {\n",
    "        \"limit\": count,\n",
    "        \"offset\": offset,\n",
    "        \"order_by\": \"-hotness\",\n",
    "        \"forecast_type\": \",\".join(\n",
    "            [\n",
    "                \"binary\",\n",
    "                \"multiple_choice\",\n",
    "                \"numeric\",\n",
    "                \"discrete\",\n",
    "            ]\n",
    "        ),\n",
    "        \"tournaments\": [tournament_id],\n",
    "        \"statuses\": \"open\",\n",
    "        \"include_description\": \"true\",\n",
    "    }\n",
    "    url = f\"{API_BASE_URL}/posts/\"\n",
    "    response = requests.get(url, **AUTH_HEADERS, params=url_qparams)  # type: ignore\n",
    "    if not response.ok:\n",
    "        raise Exception(response.text)\n",
    "    data = json.loads(response.content)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_open_question_ids_from_tournament() -> list[tuple[int, int]]:\n",
    "    posts = list_posts_from_tournament()\n",
    "\n",
    "    post_dict = dict()\n",
    "    for post in posts[\"results\"]:\n",
    "        if question := post.get(\"question\"):\n",
    "            # single question post\n",
    "            post_dict[post[\"id\"]] = [question]\n",
    "\n",
    "    open_question_id_post_id = []  # [(question_id, post_id)]\n",
    "    for post_id, questions in post_dict.items():\n",
    "        for question in questions:\n",
    "            if question.get(\"status\") == \"open\":\n",
    "                print(\n",
    "                    f\"ID: {question['id']}\\nQ: {question['title']}\\nCloses: \"\n",
    "                    f\"{question['scheduled_close_time']}\"\n",
    "                )\n",
    "                open_question_id_post_id.append((question[\"id\"], post_id))\n",
    "\n",
    "    return open_question_id_post_id\n",
    "\n",
    "\n",
    "def get_post_details(post_id: int) -> dict:\n",
    "    url = f\"{API_BASE_URL}/posts/{post_id}/\"\n",
    "    print(f\"Getting details for {url}\")\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        **AUTH_HEADERS,  # type: ignore\n",
    "    )\n",
    "    if not response.ok:\n",
    "        raise Exception(response.text)\n",
    "    details = json.loads(response.content)\n",
    "    return details\n",
    "\n",
    "CONCURRENT_REQUESTS_LIMIT = 5\n",
    "llm_rate_limiter = asyncio.Semaphore(CONCURRENT_REQUESTS_LIMIT)\n",
    "\n",
    "\n",
    "async def call_llm(prompt: str, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Makes a streaming completion request to OpenRouter's API.\n",
    "    \"\"\"\n",
    "    client = AsyncOpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "    )\n",
    "\n",
    "    async with llm_rate_limiter:\n",
    "        track_event(\"LLM Call Started\", {\"prompt_length\": len(prompt)})\n",
    "        response = await client.chat.completions.create(\n",
    "            model=OPENROUTER_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            stream=False,\n",
    "            extra_headers={\n",
    "                \"HTTP-Referer\": APP_URL, \n",
    "                \"X-Title\": APP_TITLE,\n",
    "            }\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        if answer is None:\n",
    "            track_event(\"LLM Call Failed\", {\"reason\": \"No answer returned\"})\n",
    "            raise ValueError(\"No answer returned from LLM\")\n",
    "        \n",
    "        track_event(\"LLM Call Succeeded\", {\"response_length\": len(answer)})\n",
    "        return answer\n",
    "\n",
    "# --- NEW Research Functions ---\n",
    "def call_tavily(question: str) -> str:\n",
    "    \"\"\"Performs a web search using Tavily and returns a formatted string of results.\"\"\"\n",
    "    if not tavily_client:\n",
    "        print(\"[Tavily] Tavily API key not set. Skipping search.\")\n",
    "        return \"Tavily search not performed.\"\n",
    "    try:\n",
    "        response = tavily_client.search(query=question, search_depth=\"advanced\")\n",
    "        results = \"\\n\".join([f\"- {r['content']}\" for r in response['results']])\n",
    "        return f\"Web Search Results:\\n{results}\"\n",
    "    except Exception as e:\n",
    "        return f\"Tavily search failed: {e}\"\n",
    "\n",
    "def call_newsapi(question: str) -> str:\n",
    "    \"\"\"Fetches recent news articles from NewsAPI and returns a formatted string.\"\"\"\n",
    "    if not newsapi_client:\n",
    "        print(\"[NewsAPI] NewsAPI key not set. Skipping news search.\")\n",
    "        return \"NewsAPI search not performed.\"\n",
    "    try:\n",
    "        all_articles = newsapi_client.get_everything(\n",
    "            q=question,\n",
    "            language='en',\n",
    "            sort_by='relevancy',\n",
    "            page_size=5\n",
    "        )\n",
    "        if not all_articles or not all_articles['articles']:\n",
    "            return \"No recent news articles found.\"\n",
    "\n",
    "        formatted_articles = \"\"\n",
    "        for article in all_articles['articles']:\n",
    "            formatted_articles += f\"- Title: {article['title']}\\n  Source: {article['source']['name']}\\n  Snippet: {article['description']}\\n\"\n",
    "        return f\"Recent News:\\n{formatted_articles}\"\n",
    "    except Exception as e:\n",
    "        return f\"NewsAPI search failed: {e}\"\n",
    "\n",
    "def run_research(question: str) -> str:\n",
    "    \"\"\"Orchestrates research calls to Tavily and NewsAPI and combines results.\"\"\"\n",
    "    track_event(\"Research Started\", {\"question\": question})\n",
    "    \n",
    "    tavily_results = call_tavily(question)\n",
    "    news_results = call_newsapi(question)\n",
    "    \n",
    "    research = f\"{tavily_results}\\n\\n{news_results}\"\n",
    "    \n",
    "    print(f\"########################\\nResearch Found:\\n{research}\\n########################\")\n",
    "    track_event(\"Research Completed\", {\"research_length\": len(research)})\n",
    "    return research\n",
    "\n",
    "############### BINARY ###############\n",
    "\n",
    "BINARY_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a professional forecaster interviewing for a job.\n",
    "\n",
    "Your interview question is:\n",
    "{title}\n",
    "\n",
    "Question background:\n",
    "{background}\n",
    "\n",
    "\n",
    "This question's outcome will be determined by the specific criteria below. These criteria have not yet been satisfied:\n",
    "{resolution_criteria}\n",
    "\n",
    "{fine_print}\n",
    "\n",
    "\n",
    "Your research assistant says:\n",
    "{summary_report}\n",
    "\n",
    "Today is {today}.\n",
    "\n",
    "Before answering you write:\n",
    "(a) The time left until the outcome to the question is known.\n",
    "(b) The status quo outcome if nothing changed.\n",
    "(c) A brief description of a scenario that results in a No outcome.\n",
    "(d) A brief description of a scenario that results in a Yes outcome.\n",
    "\n",
    "You write your rationale remembering that good forecasters put extra weight on the status quo outcome since the world changes slowly most of the time.\n",
    "\n",
    "The last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_probability_from_response_as_percentage_not_decimal(\n",
    "    forecast_text: str,\n",
    ") -> float:\n",
    "    matches = re.findall(r\"(\\d+)%\", forecast_text)\n",
    "    if matches:\n",
    "        number = int(matches[-1])\n",
    "        number = min(99, max(1, number))  # clamp the number between 1 and 99\n",
    "        return number\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract prediction from response: {forecast_text}\")\n",
    "\n",
    "\n",
    "async def get_binary_gpt_prediction(\n",
    "    question_details: dict, num_runs: int\n",
    ") -> tuple[float, str]:\n",
    "\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    title = question_details[\"title\"]\n",
    "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
    "    background = question_details[\"description\"]\n",
    "    fine_print = question_details[\"fine_print\"]\n",
    "\n",
    "    summary_report = run_research(title)\n",
    "\n",
    "    content = BINARY_PROMPT_TEMPLATE.format(\n",
    "        title=title,\n",
    "        today=today,\n",
    "        background=background,\n",
    "        resolution_criteria=resolution_criteria,\n",
    "        fine_print=fine_print,\n",
    "        summary_report=summary_report,\n",
    "    )\n",
    "\n",
    "    async def get_rationale_and_probability(content: str) -> tuple[float, str]:\n",
    "        rationale = await call_llm(content)\n",
    "\n",
    "        probability = extract_probability_from_response_as_percentage_not_decimal(\n",
    "            rationale\n",
    "        )\n",
    "        comment = (\n",
    "            f\"Extracted Probability: {probability}%\\n\\nGPT's Answer: \"\n",
    "            f\"{rationale}\\n\\n\\n\"\n",
    "        )\n",
    "        return probability, comment\n",
    "\n",
    "    probability_and_comment_pairs = await asyncio.gather(\n",
    "        *[get_rationale_and_probability(content) for _ in range(num_runs)]\n",
    "    )\n",
    "    comments = [pair[1] for pair in probability_and_comment_pairs]\n",
    "    final_comment_sections = [\n",
    "        f\"## Rationale {i+1}\\n{comment}\" for i, comment in enumerate(comments)\n",
    "    ]\n",
    "    probabilities = [pair[0] for pair in probability_and_comment_pairs]\n",
    "    median_probability = float(np.median(probabilities)) / 100\n",
    "\n",
    "    final_comment = f\"Median Probability: {median_probability}\\n\\n\" + \"\\n\\n\".join(\n",
    "        final_comment_sections\n",
    "    )\n",
    "    return median_probability, final_comment\n",
    "\n",
    "\n",
    "####################### NUMERIC ###############\n",
    "NUMERIC_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a professional forecaster interviewing for a job.\n",
    "\n",
    "Your interview question is:\n",
    "{title}\n",
    "\n",
    "Background:\n",
    "{background}\n",
    "\n",
    "{resolution_criteria}\n",
    "\n",
    "{fine_print}\n",
    "\n",
    "Units for answer: {units}\n",
    "\n",
    "Your research assistant says:\n",
    "{summary_report}\n",
    "\n",
    "Today is {today}.\n",
    "\n",
    "{lower_bound_message}\n",
    "{upper_bound_message}\n",
    "\n",
    "\n",
    "Formatting Instructions:\n",
    "- Please notice the units requested (e.g. whether you represent a number as 1,000,000 or 1m).\n",
    "- Never use scientific notation.\n",
    "- Always start with a smaller number (more negative if negative) and then increase from there\n",
    "\n",
    "Before answering you write:\n",
    "(a) The time left until the outcome to the question is known.\n",
    "(b) The outcome if nothing changed.\n",
    "(c) The outcome if the current trend continued.\n",
    "(d) The expectations of experts and markets.\n",
    "(e) A brief description of an unexpected scenario that results in a low outcome.\n",
    "(f) A brief description of an unexpected scenario that results in a high outcome.\n",
    "\n",
    "You remind yourself that good forecasters are humble and set wide 90/10 confidence intervals to account for unknown unkowns.\n",
    "\n",
    "The last thing you write is your final answer as:\n",
    "\"\n",
    "Percentile 10: XX\n",
    "Percentile 20: XX\n",
    "Percentile 40: XX\n",
    "Percentile 60: XX\n",
    "Percentile 80: XX\n",
    "Percentile 90: XX\n",
    "\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_percentiles_from_response(forecast_text: str) -> dict:\n",
    "\n",
    "    def extract_percentile_numbers(text) -> dict:\n",
    "        pattern = r\"^.*(?:P|p)ercentile.*$\"\n",
    "        number_pattern = r\"-\\s*(?:[^\\d\\-]*\\s*)?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)|(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\"\n",
    "        results = []\n",
    "\n",
    "        for line in text.split(\"\\n\"):\n",
    "            if re.match(pattern, line):\n",
    "                numbers = re.findall(number_pattern, line)\n",
    "                numbers_no_commas = [\n",
    "                    next(num for num in match if num).replace(\",\", \"\")\n",
    "                    for match in numbers\n",
    "                ]\n",
    "                numbers = [\n",
    "                    float(num) if \".\" in num else int(num)\n",
    "                    for num in numbers_no_commas\n",
    "                ]\n",
    "                if len(numbers) > 1:\n",
    "                    first_number = numbers[0]\n",
    "                    last_number = numbers[-1]\n",
    "                    if \"-\" in line.split(\":\")[-1]:\n",
    "                        last_number = -abs(last_number)\n",
    "                    results.append((first_number, last_number))\n",
    "\n",
    "        percentile_values = {}\n",
    "        for first_num, second_num in results:\n",
    "            key = first_num\n",
    "            percentile_values[key] = second_num\n",
    "\n",
    "        return percentile_values\n",
    "\n",
    "    percentile_values = extract_percentile_numbers(forecast_text)\n",
    "\n",
    "    if len(percentile_values) > 0:\n",
    "        return percentile_values\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract prediction from response: {forecast_text}\")\n",
    "\n",
    "\n",
    "def generate_continuous_cdf(\n",
    "    percentile_values: dict,\n",
    "    question_type: str,\n",
    "    open_upper_bound: bool,\n",
    "    open_lower_bound: bool,\n",
    "    upper_bound: float,\n",
    "    lower_bound: float,\n",
    "    zero_point: float | None,\n",
    "    cdf_size: int,\n",
    ") -> list[float]:\n",
    "    percentile_max = max(float(key) for key in percentile_values.keys())\n",
    "    percentile_min = min(float(key) for key in percentile_values.keys())\n",
    "    range_min = lower_bound\n",
    "    range_max = upper_bound\n",
    "    range_size = range_max - range_min\n",
    "    buffer = 1 if range_size > 100 else 0.01 * range_size\n",
    "\n",
    "    for percentile, value in list(percentile_values.items()):\n",
    "        if not open_lower_bound and value <= range_min + buffer:\n",
    "            percentile_values[percentile] = range_min + buffer\n",
    "        if not open_upper_bound and value >= range_max - buffer:\n",
    "            percentile_values[percentile] = range_max - buffer\n",
    "\n",
    "    if open_upper_bound:\n",
    "        if range_max > percentile_values[percentile_max]:\n",
    "            percentile_values[int(100 - (0.5 * (100 - percentile_max)))] = range_max\n",
    "    else:\n",
    "        percentile_values[100] = range_max\n",
    "\n",
    "    if open_lower_bound:\n",
    "        if range_min < percentile_values[percentile_min]:\n",
    "            percentile_values[int(0.5 * percentile_min)] = range_min\n",
    "    else:\n",
    "        percentile_values[0] = range_min\n",
    "\n",
    "    sorted_percentile_values = dict(sorted(percentile_values.items()))\n",
    "\n",
    "    normalized_percentile_values = {}\n",
    "    for key, value in sorted_percentile_values.items():\n",
    "        percentile = float(key) / 100\n",
    "        normalized_percentile_values[percentile] = value\n",
    "\n",
    "\n",
    "    value_percentiles = {\n",
    "        value: key for key, value in normalized_percentile_values.items()\n",
    "    }\n",
    "\n",
    "    def generate_cdf_locations(range_min, range_max, zero_point):\n",
    "        if zero_point is None:\n",
    "            scale = lambda x: range_min + (range_max - range_min) * x\n",
    "        else:\n",
    "            deriv_ratio = (range_max - zero_point) / (range_min - zero_point)\n",
    "            scale = lambda x: range_min + (range_max - range_min) * (\n",
    "                deriv_ratio**x - 1\n",
    "            ) / (deriv_ratio - 1)\n",
    "        return [scale(x) for x in np.linspace(0, 1, cdf_size)]\n",
    "\n",
    "    cdf_xaxis = generate_cdf_locations(range_min, range_max, zero_point)\n",
    "\n",
    "    def linear_interpolation(x_values, xy_pairs):\n",
    "        sorted_pairs = sorted(xy_pairs.items())\n",
    "\n",
    "        known_x = [pair[0] for pair in sorted_pairs]\n",
    "        known_y = [pair[1] for pair in sorted_pairs]\n",
    "\n",
    "        y_values = []\n",
    "\n",
    "        for x in x_values:\n",
    "            if x in known_x:\n",
    "                y_values.append(known_y[known_x.index(x)])\n",
    "            else:\n",
    "                i = 0\n",
    "                while i < len(known_x) and known_x[i] < x:\n",
    "                    i += 1\n",
    "\n",
    "                if i == 0:\n",
    "                    y_values.append(known_y[0])\n",
    "                elif i == len(known_x):\n",
    "                    y_values.append(known_y[-1])\n",
    "                else:\n",
    "                    x0, x1 = known_x[i - 1], known_x[i]\n",
    "                    y0, y1 = known_y[i - 1], known_y[i]\n",
    "                    y = y0 + (x - x0) * (y1 - y0) / (x1 - x0)\n",
    "                    y_values.append(y)\n",
    "\n",
    "        return y_values\n",
    "\n",
    "    continuous_cdf = linear_interpolation(cdf_xaxis, value_percentiles)\n",
    "    return continuous_cdf\n",
    "\n",
    "\n",
    "async def get_numeric_gpt_prediction(\n",
    "    question_details: dict, num_runs: int\n",
    ") -> tuple[list[float], str]:\n",
    "\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    title = question_details[\"title\"]\n",
    "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
    "    background = question_details[\"description\"]\n",
    "    fine_print = question_details[\"fine_print\"]\n",
    "    question_type = question_details[\"type\"]\n",
    "    scaling = question_details[\"scaling\"]\n",
    "    open_upper_bound = question_details[\"open_upper_bound\"]\n",
    "    open_lower_bound = question_details[\"open_lower_bound\"]\n",
    "    unit_of_measure = question_details[\"unit\"] if question_details[\"unit\"] else \"Not stated (please infer this)\"\n",
    "    upper_bound = scaling[\"range_max\"]\n",
    "    lower_bound = scaling[\"range_min\"]\n",
    "    zero_point = scaling[\"zero_point\"]\n",
    "    if question_type == \"discrete\":\n",
    "        outcome_count = question_details[\"scaling\"][\"inbound_outcome_count\"]\n",
    "        cdf_size = outcome_count + 1\n",
    "    else:\n",
    "        cdf_size = 201\n",
    "\n",
    "    if open_upper_bound:\n",
    "        upper_bound_message = \"\"\n",
    "    else:\n",
    "        upper_bound_message = f\"The outcome can not be higher than {upper_bound}.\"\n",
    "    if open_lower_bound:\n",
    "        lower_bound_message = \"\"\n",
    "    else:\n",
    "        lower_bound_message = f\"The outcome can not be lower than {lower_bound}.\"\n",
    "\n",
    "    summary_report = run_research(title)\n",
    "\n",
    "    content = NUMERIC_PROMPT_TEMPLATE.format(\n",
    "        title=title,\n",
    "        today=today,\n",
    "        background=background,\n",
    "        resolution_criteria=resolution_criteria,\n",
    "        fine_print=fine_print,\n",
    "        summary_report=summary_report,\n",
    "        lower_bound_message=lower_bound_message,\n",
    "        upper_bound_message=upper_bound_message,\n",
    "        units=unit_of_measure,\n",
    "    )\n",
    "\n",
    "    async def ask_llm_to_get_cdf(content: str) -> tuple[list[float], str]:\n",
    "        rationale = await call_llm(content)\n",
    "        percentile_values = extract_percentiles_from_response(rationale)\n",
    "\n",
    "        comment = (\n",
    "            f\"Extracted Percentile_values: {percentile_values}\\n\\nGPT's Answer: \"\n",
    "            f\"{rationale}\\n\\n\\n\"\n",
    "        )\n",
    "\n",
    "        cdf = generate_continuous_cdf(\n",
    "            percentile_values,\n",
    "            question_type,\n",
    "            open_upper_bound,\n",
    "            open_lower_bound,\n",
    "            upper_bound,\n",
    "            lower_bound,\n",
    "            zero_point,\n",
    "            cdf_size,\n",
    "        )\n",
    "\n",
    "        return cdf, comment\n",
    "\n",
    "    cdf_and_comment_pairs = await asyncio.gather(\n",
    "        *[ask_llm_to_get_cdf(content) for _ in range(num_runs)]\n",
    "    )\n",
    "    comments = [pair[1] for pair in cdf_and_comment_pairs]\n",
    "    final_comment_sections = [\n",
    "        f\"## Rationale {i+1}\\n{comment}\" for i, comment in enumerate(comments)\n",
    "    ]\n",
    "    cdfs: list[list[float]] = [pair[0] for pair in cdf_and_comment_pairs]\n",
    "    all_cdfs = np.array(cdfs)\n",
    "    median_cdf: list[float] = np.median(all_cdfs, axis=0).tolist()\n",
    "\n",
    "    final_comment = f\"Median CDF: `{str(median_cdf)[:100]}...`\\n\\n\" + \"\\n\\n\".join(\n",
    "        final_comment_sections\n",
    "    )\n",
    "    return median_cdf, final_comment\n",
    "\n",
    "\n",
    "########################## MULTIPLE CHOICE ###############\n",
    "MULTIPLE_CHOICE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a professional forecaster interviewing for a job.\n",
    "\n",
    "Your interview question is:\n",
    "{title}\n",
    "\n",
    "The options are: {options}\n",
    "\n",
    "\n",
    "Background:\n",
    "{background}\n",
    "\n",
    "{resolution_criteria}\n",
    "\n",
    "{fine_print}\n",
    "\n",
    "\n",
    "Your research assistant says:\n",
    "{summary_report}\n",
    "\n",
    "Today is {today}.\n",
    "\n",
    "Before answering you write:\n",
    "(a) The time left until the outcome to the question is known.\n",
    "(b) The status quo outcome if nothing changed.\n",
    "(c) A description of an scenario that results in an unexpected outcome.\n",
    "\n",
    "You write your rationale remembering that (1) good forecasters put extra weight on the status quo outcome since the world changes slowly most of the time, and (2) good forecasters leave some moderate probability on most options to account for unexpected outcomes.\n",
    "\n",
    "The last thing you write is your final probabilities for the N options in this order {options} as:\n",
    "Option_A: Probability_A\n",
    "Option_B: Probability_B\n",
    "...\n",
    "Option_N: Probability_N\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_option_probabilities_from_response(forecast_text: str, options) -> float:\n",
    "    def extract_option_probabilities(text):\n",
    "\n",
    "        number_pattern = r\"-?\\d+(?:,\\d{3})*(?:\\.\\d+)?\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for line in text.split(\"\\n\"):\n",
    "            numbers = re.findall(number_pattern, line)\n",
    "            numbers_no_commas = [num.replace(\",\", \"\") for num in numbers]\n",
    "            numbers = [\n",
    "                float(num) if \".\" in num else int(num) for num in numbers_no_commas\n",
    "            ]\n",
    "            if len(numbers) >= 1:\n",
    "                last_number = numbers[-1]\n",
    "                results.append(last_number)\n",
    "\n",
    "        return results\n",
    "\n",
    "    option_probabilities = extract_option_probabilities(forecast_text)\n",
    "\n",
    "    NUM_OPTIONS = len(options)\n",
    "\n",
    "    if len(option_probabilities) > 0:\n",
    "        return option_probabilities[-NUM_OPTIONS:]\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract prediction from response: {forecast_text}\")\n",
    "\n",
    "\n",
    "def generate_multiple_choice_forecast(options, option_probabilities) -> dict:\n",
    "    if len(options) != len(option_probabilities):\n",
    "        raise ValueError(\n",
    "            f\"Number of options ({len(options)}) does not match number of probabilities ({len(option_probabilities)})\"\n",
    "        )\n",
    "\n",
    "    total_sum = sum(option_probabilities)\n",
    "    if total_sum == 0:\n",
    "        # Avoid division by zero if all probabilities are 0\n",
    "        return {option: 1/len(options) for option in options} \n",
    "\n",
    "    decimal_list = [x / total_sum for x in option_probabilities]\n",
    "\n",
    "    def normalize_list(float_list):\n",
    "        clamped_list = [max(min(x, 0.99), 0.01) for x in float_list]\n",
    "        total_sum = sum(clamped_list)\n",
    "        normalized_list = [x / total_sum for x in clamped_list]\n",
    "        adjustment = 1.0 - sum(normalized_list)\n",
    "        normalized_list[-1] += adjustment\n",
    "        return normalized_list\n",
    "\n",
    "    normalized_option_probabilities = normalize_list(decimal_list)\n",
    "\n",
    "    probability_yes_per_category = {}\n",
    "    for i in range(len(options)):\n",
    "        probability_yes_per_category[options[i]] = normalized_option_probabilities[i]\n",
    "\n",
    "    return probability_yes_per_category\n",
    "\n",
    "\n",
    "async def get_multiple_choice_gpt_prediction(\n",
    "    question_details: dict,\n",
    "    num_runs: int,\n",
    ") -> tuple[dict[str, float], str]:\n",
    "\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    title = question_details[\"title\"]\n",
    "    resolution_criteria = question_details[\"resolution_criteria\"]\n",
    "    background = question_details[\"description\"]\n",
    "    fine_print = question_details[\"fine_print\"]\n",
    "    options = question_details[\"options\"]\n",
    "\n",
    "    summary_report = run_research(title)\n",
    "\n",
    "    content = MULTIPLE_CHOICE_PROMPT_TEMPLATE.format(\n",
    "        title=title,\n",
    "        today=today,\n",
    "        background=background,\n",
    "        resolution_criteria=resolution_criteria,\n",
    "        fine_print=fine_print,\n",
    "        summary_report=summary_report,\n",
    "        options=options,\n",
    "    )\n",
    "\n",
    "    async def ask_llm_for_multiple_choice_probabilities(\n",
    "        content: str,\n",
    "    ) -> tuple[dict[str, float], str]:\n",
    "        rationale = await call_llm(content)\n",
    "\n",
    "        option_probabilities = extract_option_probabilities_from_response(\n",
    "            rationale, options\n",
    "        )\n",
    "\n",
    "        comment = (\n",
    "            f\"EXTRACTED_PROBABILITIES: {option_probabilities}\\n\\nGPT's Answer: \"\n",
    "            f\"{rationale}\\n\\n\\n\"\n",
    "        )\n",
    "\n",
    "        probability_yes_per_category = generate_multiple_choice_forecast(\n",
    "            options, option_probabilities\n",
    "        )\n",
    "        return probability_yes_per_category, comment\n",
    "\n",
    "    probability_yes_per_category_and_comment_pairs = await asyncio.gather(\n",
    "        *[ask_llm_for_multiple_choice_probabilities(content) for _ in range(num_runs)]\n",
    "    )\n",
    "    comments = [pair[1] for pair in probability_yes_per_category_and_comment_pairs]\n",
    "    final_comment_sections = [\n",
    "        f\"## Rationale {i+1}\\n{comment}\" for i, comment in enumerate(comments)\n",
    "    ]\n",
    "    probability_yes_per_category_dicts: list[dict[str, float]] = [\n",
    "        pair[0] for pair in probability_yes_per_category_and_comment_pairs\n",
    "    ]\n",
    "    average_probability_yes_per_category: dict[str, float] = {}\n",
    "    for option in options:\n",
    "        probabilities_for_current_option: list[float] = [\n",
    "            d[option] for d in probability_yes_per_category_dicts\n",
    "        ]\n",
    "        average_probability_yes_per_category[option] = sum(\n",
    "            probabilities_for_current_option\n",
    "        ) / len(probabilities_for_current_option)\n",
    "\n",
    "    final_comment = (\n",
    "        f\"Average Probability Yes Per Category: `{average_probability_yes_per_category}`\\n\\n\"\n",
    "        + \"\\n\\n\".join(final_comment_sections)\n",
    "    )\n",
    "    return average_probability_yes_per_category, final_comment\n",
    "\n",
    "\n",
    "################### FORECASTING ###################\n",
    "def forecast_is_already_made(post_details: dict) -> bool:\n",
    "    try:\n",
    "        forecast_values = post_details[\"question\"][\"my_forecasts\"][\"latest\"][\n",
    "            \"forecast_values\"\n",
    "        ]\n",
    "        return forecast_values is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "async def forecast_individual_question(\n",
    "    question_id: int,\n",
    "    post_id: int,\n",
    "    submit_prediction: bool,\n",
    "    num_runs_per_question: int,\n",
    "    skip_previously_forecasted_questions: bool,\n",
    ") -> str:\n",
    "    post_details = get_post_details(post_id)\n",
    "    question_details = post_details[\"question\"]\n",
    "    title = question_details[\"title\"]\n",
    "    question_type = question_details[\"type\"]\n",
    "\n",
    "    track_event(\"Forecast Started\", {\n",
    "        \"question_id\": question_id,\n",
    "        \"post_id\": post_id,\n",
    "        \"question_title\": title,\n",
    "        \"question_type\": question_type\n",
    "    })\n",
    "\n",
    "    summary_of_forecast = \"\"\n",
    "    summary_of_forecast += f\"-----------------------------------------------\\nQuestion: {title}\\n\"\n",
    "    summary_of_forecast += f\"URL: https://www.metaculus.com/questions/{post_id}/\\n\"\n",
    "\n",
    "    if question_type == \"multiple_choice\":\n",
    "        options = question_details[\"options\"]\n",
    "        summary_of_forecast += f\"options: {options}\\n\"\n",
    "\n",
    "    if (\n",
    "        forecast_is_already_made(post_details)\n",
    "        and skip_previously_forecasted_questions == True\n",
    "    ):\n",
    "        summary_of_forecast += f\"Skipped: Forecast already made\\n\"\n",
    "        track_event(\"Forecast Skipped\", {\"question_id\": question_id, \"reason\": \"Already forecast\"})\n",
    "        return summary_of_forecast\n",
    "\n",
    "    if question_type == \"binary\":\n",
    "        forecast, comment = await get_binary_gpt_prediction(\n",
    "            question_details, num_runs_per_question\n",
    "        )\n",
    "    elif question_type in [\"numeric\", \"discrete\"]:\n",
    "        forecast, comment = await get_numeric_gpt_prediction(\n",
    "            question_details, num_runs_per_question\n",
    "        )\n",
    "    elif question_type == \"multiple_choice\":\n",
    "        forecast, comment = await get_multiple_choice_gpt_prediction(\n",
    "            question_details, num_runs_per_question\n",
    "        )\n",
    "    else:\n",
    "        track_event(\"Forecast Failed\", {\"question_id\": question_id, \"reason\": f\"Unknown question type: {question_type}\"})\n",
    "        raise ValueError(f\"Unknown question type: {question_type}\")\n",
    "\n",
    "    print(f\"-----------------------------------------------\\nPost {post_id} Question {question_id}:\\n\")\n",
    "    print(f\"Forecast for post {post_id} (question {question_id}):\\n{forecast}\")\n",
    "    print(f\"Comment for post {post_id} (question {question_id}):\\n{comment[:300]}...\")\n",
    "\n",
    "    if question_type in [\"numeric\", \"discrete\"]:\n",
    "        summary_of_forecast += f\"Forecast: {str(forecast)[:200]}...\\n\"\n",
    "    else:\n",
    "        summary_of_forecast += f\"Forecast: {forecast}\\n\"\n",
    "\n",
    "    summary_of_forecast += f\"Comment:\\n
